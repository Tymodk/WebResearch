# The Explanation about the Demo's in this REPO
TODO: Explain it!
## Behold the Learning curve:
<img src="https://cdn-images-1.medium.com/max/800/1*sK6hjHszCwTE8GqtKNe1Yg.png">
A bit of explanation about the sigmoid curve. 

The 'Neuron' will be given a certain weight on each input (A positive or negative number). Depending on the weight on the input, the Neuron will behave differently. We then pass the weights through the Sigmoid function which will normalize it between 0 and 1. The gradient at the ends of the curve indicate how confident we are about the existing weights.  
<img src="https://cdn-images-1.medium.com/max/400/1*-1trgA6DUEaafJZv3k0mGw.jpeg">
This is how a neuron thinks. It takes inputs from the training set, adjust them by weights and then run them through a special formula to calculate the neuron's input.  
It will then calculate the error, which is the difference between the output of the neuron and the desired output of the training set.
Depending on the direction of the error, it'll adjust its weights. 
This will then be repeated about 10,000 times!

Once done it can then be given a completely new set of inputs, and based on its previous knowledge it should be able to make a good prediction on what the solution ought to be!
This is back propagation.	


Here are some cool demos that utilize Deep Learning: 
</br><a href="https://cs.stanford.edu/people/karpathy/convnetjs/demo/image_regression.html">Image Drawing Demo</a>
